## 第一章

# 利用神经网络识别手写数字

人类的视觉系统是世界奇迹之一。考虑下面的手写数字序列：

<img src="http://neuralnetworksanddeeplearning.com/images/digits.png" style="zoom:30%" />

大多数人毫不费力地将这些数字识别为504192.这种毫不费力是欺骗性的。在我们大脑的每个半球中，人类都有一个主要的视觉皮层，也被称为V1，含有1.4亿个神经元，它们之间有数以百亿计的联系。然而，人类视觉不仅涉及V1，而且包括V2，V3，V4和V5在内的整个视觉皮质系列 - 逐渐进行更复杂的图像处理。我们的头脑中有一台超级计算机，根据数亿年的演变进行调整，并且非常适合了解视觉世界。识别手写数字并不容易。相反，我们人类是惊人的，非常擅长理解我们的眼睛向我们展示的东西。但几乎所有这些工作都是在无意识中完成的。所以我们通常不会意识到我们的视觉系统解决问题有多艰难。

如果您尝试编写计算机程序来识别上述数字，则视觉模式识别的难度就会变得明显。看似容易，然而当我们自己做时，你会发现突然之间这变得非常困难。我们如何识别形状最简单的直觉是 - “一个9在顶部有一个圈，而在右下方有一竖” - 这种结果并不是那么简单地可以在算法上表达。当你试图精确地制定这样的规则时，你很快会迷失在各种例外以及特殊情况之中。而这似乎解决不了什么问题。

神经网络以不同的方式解决问题。这个想法是采取大量的手写数字，称为训练示例，

<img src="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png" style="zoom:100%" />

然后开发一个可以从这些例子中学习的系统。换句话说，神经网络使用这些例子来自动推断识别手写数字的规则。此外，通过增加训练示例的数量，神经网络可以了解更多关于手写的内容，从而提高其准确性。所以，虽然我只显示了上面的100个训练数字，但也许我们可以使用数千甚至数百万或数十亿个训练示例来构建更好的手写识别器。

在本章中，我们将编写一个计算机程序，实现一个学习识别手写数字的神经网络。该程序只有74行，并且不使用特殊的神经网络库。但是这个简短的程序能够以超过96％的精度识别数字，无需人工干预。此外，在后面的章节中，我们将开发可以将准确度提高到99％以上的想法。事实上，最好的商业神经网络现在非常好，银行用它来处理支票，并由邮局来识别地址。

我们专注于手写识别，因为它对于一般学习神经网络来说是一个很好的原型问题。作为原型，==它迎来了一个甜蜜点==：它很具有挑战性 - 识别手写数字并不是一件容易的事情 - 但它并不是一个非常复杂的解决方案或者需要巨大的计算能力。此外，这是开发更深入学习等更先进技术的好方法。因此，在整本书中，我们会一再回顾手写识别的问题。在本书后面，我们将讨论这些想法如何应用于计算机视觉中的其他问题，以及语音，自然语言处理和其他领域。

当然，如果本章的要点只是编写一个识别手写数字的电脑程序，那么本章将会更短！但随后，我们将开发许多关于神经网络的关键思想，包括两种重要类型的人工神经元（感知器和S形神经元）和神经网络的标准学习算法，称为随机梯度下降。在整个过程中，我重点解释为什么事情按照他们的方式完成，并且建立你的神经网络直觉。这需要更长时间的讨论，但为了您能获得更深的理解，这是值得的。 到本章结束时，我们将理解深度学习是什么，为什么它很重要。

## 感知器

什么是神经网络？要开始，我将解释一种称为感知器的人造神经元。受到了Warren McCulloch和Walter Pitts早期工作的启发，感知器是在20世纪五六十年代由科学家Frank Rosenblatt开发的。今天，使用其他人造神经元模型更为常见 - 在本书中以及在神经网络上的大量现代工作中，使用的主要神经元模型是一种称为S形神经元的模型。我们马上就会讲到S形神经元。但要理解为什么S形神经元的定义方式是这样，这就得花时间先了解**感知器**。

那么感知器如何工作？一个感知器需要几个二进制输入，$x1$，$x2$，...，并产生一个单一的二进制输出：

<img alt="proceptrons" src="http://neuralnetworksanddeeplearning.com/images/tikz0.png">

在所示的例子中，感知器有三个输入，$x1$，$x2$，$x3$。一般来说，它可以有更多或更少的输入。Rosenblatt提出了一个简单的规则来计算输出。他介绍了*权重*$w1$，$w2$，...，这些实数表示各个输入对输出的重要性。神经元的输出0或1由加权和$\sum_j w_j x_j$是小于还是大于某个阈值决定。就像权重一样，阈值是一个实数，它是神经元的一个参数。把它放在更精确的代数术语中：

$\begin{eqnarray}\mbox{output} & = & \left\{ \begin{array}{ll}0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}\end{array} \right.\tag{1}\end{eqnarray}$

这就是感知器如何工作的一切！

这是基本的数学模型。 您可以这样去想感知器，它是一种通过权衡证据来做出决定的设备。 让我举个例子。 这不是一个非常现实的例子，但它很容易理解，我们很快就会得到更实际的例子。 假设周末即将到来，你听说你的城市将会有一场奶酪节。 你喜欢奶酪，并试图决定是否参加这个节日。 您可以通过权衡三个因素来做出决定：

1. 天气好吗？
2. 你的男朋友或女朋友是否想陪你？
3. 节日地点附近公交方便吗？ （你没有车）。

我们可以用相应的二元变量$x1$，$x2$和$x3$来表示这三个因子。 例如，如果天气好，我们有$x1 = 1$，如果天气不好，$x1 = 0$。 同样，如果你的男朋友或女朋友想要去$x2 = 1$，如果没有，$x2 = 0$。 对于$x3$和公交来说同样如此。

现在，假设你超级喜欢奶酪，那么即使你的男朋友或女朋友不感兴趣并且节日地点很难到达，你也很乐意去参加这个音乐节。但是也许你真的厌恶恶劣天气，如果天气不好，你就不可能去参加这个节日。您可以使用感知器来对这种决策进行建模。一种方法是为天气选择一个重量 $w1 = 6$，其他条件选择 $w2 = 2$ 和 $w3 = 2$ 。 $w1$的较大值表示天气对您来说很重要，远远超过您的男朋友或女朋友是否加入您或公共交通的便利程度。最后，假设你为感知器选择了一个5的阈值。通过这些选择，感知器实现所需的决策模型，在天气良好时输出1，而在天气不好时输出0。无论你的男朋友还是女朋友想去，或者公共交通是否在附近，输出结果都没有区别。

通过改变权重和阈值，我们可以得到不同的决策模型。例如，假设我们选择了3的阈值，那么感知器会决定在天气好的时候或节日公共交通便利时，您的男朋友或女朋友愿意加入您，您就去参加音乐节。换句话说，这将是一个不同的决策模式。降低门槛意味着你更愿意去参加这个节日。

显然，感知器不是一个完整的人类决策模型！但是，这个例子说明了感知器如何权衡不同种类的证据以作出决定。似乎有理由认为，复杂的感知器网络可能会做出相当微妙的决定：

<img alt="tikz1" src="http://neuralnetworksanddeeplearning.com/images/tikz1.png">

在这个网络中，第一列感知器 - 我们称之为第一**层**感知器 - 通过权衡输入证据，做出三个非常简单的决定。第二层感知器呢？这些感知器中的每一个都通过对第一层决策结果进行权衡来做出决定。通过这种方式，第二层中的感知器可以在比第一层中的感知器更复杂和更抽象的层次上做出决定。第三层感知器可以做出更复杂的决定。通过这种方式，感知器的多层网络可以进行复杂的决策。

顺便说一下，当我定义感知器时，我说感知器只有一个输出。在感知器之上的网络看起来他们有多个输出。事实上，他们仍然是单一输出。多个输出箭头仅仅是一个有用的方式，表明感知器的输出被用作其他几个感知器的输入。它比绘制一条单一的输出线然后分裂更简洁。

让我们简化描述感知器的方式。条件 $\sum _j w_jx_j > threshold$ 很麻烦，我们可以改变两个符号来简化它。第一个改变是将 $\sum _j w_jx_j$ 写成一个点积，$w \cdot x \equiv \sum_j w_j x_j$ ，其中 $w$ 和  $x$ 分别是权重和输入的向量。第二个变化是将 $threshold$ 移到不等式的另一端，并用所谓的感知器的**偏差** $b≡-threshold$ 来代替它。使用偏差而不是阈值，感知器规则可以被重写为：

$\begin{eqnarray}\mbox{output} = \left\{ \begin{array}{ll} 0 & \mbox{if } w\cdot x + b \leq 0 \\1 & \mbox{if } w\cdot x + b > 0\end{array}\right.\tag{2}\end{eqnarray}$

你可以将偏差看作是衡量让感知器输出一个1的容易程度的一个量度。或者用更多的生物学术语来说，偏差是衡量让感知器发射的容易程度。==对于一个有很大偏差的感知器，感知器输出一个1非常容易==。但是如果偏差非常小，那么感知器很难输出一个1。显然，引入偏差只是我们描述感知器一个小的变化，但我们稍后会看到它会导致进一步的符号简化。因此，在本书的其余部分我们不会使用阈值，我们将始终使用偏差。

我将感知器描述为衡量证据作出决定的一种方法。另一种可以使用感知器的方法是计算我们通常认为的底层逻辑函数作为底层计算，函数如AND，OR和NAND。例如，假设我们有一个有两个输入的感知器，每个输入的权重为-2，总体偏差为3.下面是我们的感知器：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz2.png">

然后我们看到输入00产生输出1，因为$（-2）* 0 +（ - 2）* 0 + 3 = 3$ 结果是正的。 在这里，我已经引入了$*$ 符号来使乘法显式化。 类似的计算表明输入01和10产生输出1.但是输入11产生输出0，因为$（-2）* 1 +（ - 2）* 1 + 3 = -1$是负数。 所以我们的感知器实现了与非门！

附: NAND的逻辑如下:
|  A   |  B   | A NAND B |
| :--: | :--: | :------: |
|  0   |  0   |    1     |
|  0   |  1   |    1     |
|  1   |  0   |    1     |
|  1   |  1   |    0     |

NAND示例显示我们可以使用感知器来计算简单的逻辑函数。 实际上，我们可以使用感知器网络来计算任何逻辑函数。 原因在于与非门的计算通用的，也就是说，我们可以在NAND门外建立任何计算。 例如，我们可以使用与非门来构建一个电路，该电路对两个位$x1$和$x2$求和。 这需要计算按位和$x1⊕x2$以及当$x1$和$x2$都为1时进位被设置为1，即进位只是$x1, x2$的按位乘积：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz3.png">

为了获得感知器的等效网络，我们用感知器替换了所有与非门，其中两个输入均为-2，总体偏差为3.这就是最终的网络。 请注意，我已经将感应器与底部右侧的“与非”门相对应移动了一点，以便更容易地绘制图上的箭头：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz4.png">

这个感知器网络的一个值得注意的方面是，最左感知器的输出被用作最下感知器的两倍输入。 当我定义感知器模型时，我没有说是否允许这种双重输出到同一个地方。 其实，这并不重要。 如果我们不想允许这样的事情，那么可以简单地将这两条线合并成一个权重为-4的单一连接，而不是两个具有-2权重的连接。 （如果你没有发现这一点，你应该停下来向自己证明这是等同的）。通过这种变化，网络看起来如下，所有未标记的权重等于-2，所有偏差等于3，并且有一个权重为-4，如下所示：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz5.png">

到目前为止，我一直在绘制像$x1$和$x2$这样的输入作为漂浮在感知器网络左侧的变量。 事实上，传统的做法是绘制额外的感知层：**输入层**  来对输入进行编码：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz6.png">

输入感知器的下面这种表示法，其中我们有输出，但没有输入，

<img src="http://neuralnetworksanddeeplearning.com/images/tikz7.png">

这只是一种简写。它实际上并不意味着这是没有输入的感知器。 要注意到，假设我们确实有一个没有输入的感知器。 那么加权总和 $\sum_j w_j x_j$ 总是为零，所以感知器如果b> 0将输出1，如果 b≤0 则输出0。 也就是说，感知器只会输出一个固定的值，而不是所需的值（在上面的例子中b是 $x1$）。 最好将输入感知器视为根本不是感知器，而是将其简单定义为输出所需的值特定单元。

$x1, x2, ...$

加法器实例演示了如何使用感知器网络来模拟包含许多与非门的电路。由于NAND门在计算上是通用的，因此感知器对于计算也是普遍的。

感知器的计算普遍性同时令人放心也令人失望。这令人放心，因为它告诉我们，感知器网络可以像其他任何计算设备一样强大。但它也令人失望，因为它使得它看起来好像感知器仅仅是一种新型的NAND门。这不是什么大新闻！

但是，情况比这个观点表明的要好。事实证明，我们可以设计出可以自动调整人造神经元网络的权重和偏差的学习算法。这种调整是针对==外部刺激==而发生的，没有程序员的直接干预。这些学习算法使我们能够用与传统逻辑门完全不同的方式使用人造神经元。我们的神经网络不是明确地布局NAND电路和其他门电路，而是简单地学会解决问题，有时会遇到直接设计传统电路极其困难的问题。

# Sigmoid 神经元

学习算法听起来很棒。 但是我们怎样才能为神经网络设计这样的算法呢？ 假设我们有一个我们想用来学习解决某个问题的感知器网络。 例如，网络的输入可能是来自手写的数字扫描图像的原始像素数据。 我们希望网络学习权重和偏差，以便网络输出正确地对数字进行分类。 要了解学习如何起作用，假设我们对网络中的一些权重（或偏差）进行小改动。 我们想要的是，这种小的权重变化只会导致网络输出的相应变化很小。 正如我们稍后会看到的，这个属性将使学习成为可能。 简单来说，这就是我们想要的（显然这个网络太简单了，无法进行手写识别！）：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz8.png">

如果权重（或偏差）的微小变化确实只会导致产出的微小变化，那么我们可以利用这个事实来修改权重和偏差，从而使我们的网络以我们想要的方式变化。例如，假设当网络应该是“9”时，网络错误地将图像分类为“8”。我们可以弄清楚如何对权重和偏差做一个小的改变，这样网络就会更接近将图像分类为“9”。然后我们再重复一遍，改变重量和偏差以产生更好更好的输出。==网络将会是学习==。

问题是，当我们的网络包含感知器时，这不会发生什么。事实上，网络中任何单个感知器的权重或偏差的小改变有时会导致该感知器的输出完全翻转，例如从0到1.这个翻转可能导致网络其余部分的行为以一些非常复杂的方式完全改变。因此，虽然现在您的“9”可能被正确分类，但网络在所有其他图像上的行为可能会以一些难以控制的方式完全改变。这使得很难看到如何逐渐修改权重和偏差，以使网络更接近所需的行为。也许有一些解决这个问题的巧妙方法。但是，我们如何获得感知器的网络来学习并不是一蹴即就的。

我们可以通过引入一种称为S形神经元的新型人造神经元来克服这个问题。 S形神经元与感知器类似，但是经过修改使得它们的权重和偏差的小变化仅导致其输出的小变化。 这是允许一个S形神经元网络学习的关键事实。

好的，让我描述S形神经元。 我们将用描述感知器的相同方式描述S形神经元：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz9.png">

就像感知器，S形神经元有输入， $x1，x2，... $ 。 但是，不是仅仅是0或1，这些输入也可以取0和1之间的任何值。因此，例如，$0.638 ...$ 是S形神经元的有效输入。 也就像感知器一样，S形神经元对每个输入 $w1，w2，... $ 和整体偏差 $b$ 都有权重。 但输出不是 0 或 1，而是 $\sigma(w \cdot x+ b)$，其中σ被称为 *sigmoid函数* (顺便说一句，σ 有时被称为*逻辑函数*， 被称为*逻辑神经元*的新一类神经元。 记住这个术语是有用的，因为许多使用神经网络的人使用这些术语。)  但是，我们将坚持使用 *sigmoid函数*，并且定义如下：

$$\begin{eqnarray} \sigma(z) \equiv \frac{1}{1+e^{-z}}.\tag{3}\end{eqnarray}$$

为了更清楚地说明，具有输入$x1，x2，...$ 权重$w1，w2，...，$ 和偏差 $b$ 的S形神经元的输出是

$$\begin{eqnarray} \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.\tag{4}\end{eqnarray}$$

乍一看，S形神经元与感知器显得非常不同。如果你还不熟悉sigmoid函数的代数形式，可能看起来==不透明并且禁止==。事实上，感知器和S形神经元之间有许多相似之处，而S形函数的代数形式更像是技术细节而不是理解的真正障碍。

为了理解与感知器模型的相似性，假设$z≡w⋅x+ b$是一个很大的正数。那么 $e^{-z}≈0$，所以$σ(z)≈1$。换句话说，当z =w⋅x+ b很大且为正数时，S形神经元的输出大约为1，就像感知器的输出一样。另一方面，假设z =w⋅x+ b是非常负的。那么$e^{-z}→∞$，$σ(z)≈0$。所以当z =w⋅x+ b非常负时，S形神经元的行为也非常接近感知器。只有当w⋅x+ b的大小适中时，才会与感知器模型有很大的偏差。

那么σ的代数形式呢？我们如何理解？事实上，σ的确切形式并不那么重要 - **真正重要的是绘制时函数的形状**。这是形状：

<img src="http://p3pjk7oa2.bkt.clouddn.com/s_function.png">

这种形状是一个==平滑的==阶跃函数：

<img src="http://p3pjk7oa2.bkt.clouddn.com/step_function.png">

如果σ实际上是一个阶梯函数，那么S形神经元将是一个感知器，因为输出将是1或0，这取决于 $w⋅x+b$ 是正还是负 (实际上，当$w⋅ x + b =0$感知器输出0，而阶梯函数输出为1.因此，严格来说，我们需要在该点修改阶梯函数。 但你明白了。) 通过使用实际的σ函数，我们可以得到一个平滑的感知器。 的确，σ函数的平滑性是至关重要的事实，而不是其详细的形式。 σ的平滑性意味着权重中的小变化 $Δw_j$ 和偏差中的 $Δb$ 将在神经元的输出中产生小变化$Δoutput$。 事实上，微积分告诉我们 $Δoutput$ 很好地近似于

$$\begin{eqnarray} \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j} \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b, \tag{5}\end{eqnarray}$$

式子里，==求和是对所有权重== $w_j​$ 和  $\frac{∂output}{∂w_j}​$ 和$\frac{∂output}{∂b}​$分别表示输出相对于 $w_j​$ 和 $b​$ 的偏导数。 如果您不喜欢偏微分衍生产品，请不要惊慌！ 虽然上面的表达式看起来很复杂，但对于所有的偏导数，它实际上是在说一些非常简单的事情（这是一个非常好的消息）：$Δoutput​$是权重和偏差中变化 $Δw_j​$ 和 $Δb​$ 的线性函数。 这种线性使得轻松地选择权重和偏差的小变化来实现输出的任何希望的小变化。 因此，尽管S形神经元与感知器具有很多相同的定性行为，但它们使得更容易弄清如何改变权重和偏差会改变输出。

如果σ的形状是真正重要的，而不是它的确切形式，那么为什么在方程（3）中使用σ的特定形式呢？ 事实上，在本书后面我们偶尔会考虑一些其他**激活函数** ==$f(⋅)$ 的输出为$f(w⋅x+ b)$的神经元==。 当我们使用不同的激活函数时，主要的变化是方程（5）中偏导数的特定值改变。 事实证明，当我们稍后计算这些偏导数时，使用σ将简化代数，仅仅因为指数在区分时具有可爱的性质。 无论如何，σ通常用于神经网络的工作，并且是本书中最常用的激活函数。

我们应该如何解释S形神经元的输出？显然，感知器和S形神经元之间的一个很大的区别是S形神经元不仅输出0或1.他们可以输出任何0和1之间的实数，因此0.173 ...和0.689等值是合法输出。例如，如果我们想要使用输出值来表示输入到神经网络的图像中像素的平均强度，这可能很有用。但有时它可能是一个滋扰。假设我们希望来自网络的输出指示“输入图像是9”或“输入图像不是9”。显然，如果输出为0或1，那么最容易做到这一点，就像感知器一样。但实际上，我们可以设定一个约定来处理这个问题，例如，通过决定将至少0.5的任何输出解释为表示“9”，并且将小于0.5的任何输出解释为表示“不是9”来解释。我会一直明确地说出我们何时使用这样的惯例，所以它不应该引起任何混淆。

## 练习

- **S神经元模拟感知器，第一部分**

  假设我们把所有的权重和偏差放在一个感知器网络中，并把它们乘以一个正常数，c> 0。 证明网络的行为不会改变。

- **模拟感知器，第二部分**

  假设我们具有与最后一个问题相同的设置 - 感知器网络。 还假设已经选择了感知器网络的整体输入。 我们不需要实际的输入值，我们只需要输入固定。 假设权重和偏差对于网络中任何特定感知器的输入$x$，$w⋅x+ b≠0$。 现在用S形神经元代替网络中的所有感知器，并将权重和偏差乘以正常数c> 0。 证明在极限c→∞时，这个S形神经元网络的行为与感知器网络完全相同。 当一个感知器的w⋅x+ b = 0时，这怎么会失败？

## 尝试作答

- 由于$$\sum_j(cw_jx_j + cb) = c\sum_j(w_jx_j + b)$$ , 正数$c$乘以$\sum_j(w_jx_j + b)$得到的符号和$\sum_j(w_jx_j + b)$的正负符号一致。因此网络的行为不会改变。
- 当$c→-∞$时, 对于感知器网络$c\sum_j(w_jx_j + b) < 0$, 因此输出为0; 对于S形神经网络输出$$\begin{eqnarray} \frac{1}{1+\exp(c(-\sum_j w_j x_j-b))}\end{eqnarray} → 0$$。类似的，当$c→+∞$时感知网络的输出为1，S形神经网络的输出同样为1。特别地，当$w\cdot x + b = 0$时，感知神经网络的输出始终是０，而S形神经网络的输出0.5。因此除去这种特殊情况，S形神经网络的行为和感知器网络完全相同。

# 神经网络的结构

在下一节中，我将介绍一个可以对手写数字进行分类的神经网络。 为此做准备，这有助于解释一些术语，让我们可以命名网络的不同部分。 假设我们有网络：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz10.png">

如前所述，该网络中最左边的层称为输入层，层内的神经元称为输入神经元。 最右边或输出层包含输出神经元，或者，在这种情况下为单个输出神经元。 中间层被称为隐藏层，因为该层中的神经元既不是输入也不是输出。 “隐藏”这个词或许听起来有点神秘 - 第一次听到这个词我认为它必须有一些深刻的哲学或数学意义 - 但它只不过意味着“不是输入或输出”。 上面的网络只有一个隐藏层，但有些网络有多个隐藏层。 例如，以下四层网络有两个隐藏层：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz11.png">

有点令人困惑的是，由于历史原因，这种多层网络有时被称为*多层感知器*或*MLPs*，尽管是由S形神经元而不是感知器组成的。 我不打算在本书中使用MLP术语，因为我认为它很混乱，但是要提醒你它的存在。网络中输入和输出层的设计通常很简单。 例如，假设我们试图确定手写图像是否描绘了“9”。 设计网络的一种自然方式是将图像像素的强度编码成输入神经元。 如果图像是 64×64 的灰度图像，那么我们将有 4,096 = 64×64 输入神经元，其强度在0和1之间适当缩放。输出层将只包含一个神经元，并且输出 表示“输入图像不是9”的值小于0.5，并且大于0.5的值表示“输入图像是9”。

虽然神经网络的输入和输出层的设计通常是直截了当的，但==隐藏层的设计可能有相当的艺术==。 特别是，用几个简单的经验法则来总结隐藏层的设计过程是不可能的。 相反，神经网络研究人员已经为隐藏层开发了许多设计启发式方法，这些方法可以帮助人们从他们的网络中获得他们想要的行为。 例如，可以使用这种启发式方法来帮助确定如何根据训练网络所需的时间权衡隐藏层的数量。 本书稍后会介绍几种这样的设计**启发式方法**。

到目前为止，我们一直在讨论神经网络，其中一层的输出被用作下一层的输入。 这种网络被称为*前馈*神经网络。 这意味着网络中没有环路 - 信息总是前馈，永不反馈。 如果我们确实有循环，最终会出现σ函数的输入取决于输出的情况。 这很难理解，所以我们不允许这样的循环。

但是，还有其他可能带反馈回路的人工神经网络模型。 这些模型被称为[递归神经网络](http://en.wikipedia.org/wiki/Recurrent_neural_network)。 这些模型的思想是让神经元在静止之前有限的时间内被触发。 这种==触发==可以刺激其他神经元，这些神经元可能会在稍后==触发==，持续时间也有限。 这会导致更多的神经元==触发==，所以随着时间的推移，我们会得到一连串的神经元==触发==。 循环不会在这样的模型中造成问题，因为神经元的输出仅在稍后时间而不是瞬时影响其输入。

递归神经网络比前馈网络的影响力小，部分原因是递归网络的学习算法（至少到目前为止）不那么强大。 但递归神经网络仍然非常有趣。 他们更接近我们的大脑如何工作。 有可能递归神经网络可以解决重要问题，而这些问题通过前馈网络很难解决。 然而，为了限制我们的范围，在本书中，我们将专注于更广泛使用的前馈网络。

# 一个简单的网络来分类手写数字

定义了神经网络之后，让我们回到手写识别。 我们可以将识别手写数字的问题分成两个子问题。 首先，我们想要一种将包含许多数字的图像分解为一系列单独图像的方式，每个图像都包含一个数字。 例如，我们想要将下面这张图

<img src="http://neuralnetworksanddeeplearning.com/images/digits.png" width="50%">

分成六张独立的图片

<img src="http://neuralnetworksanddeeplearning.com/images/digits_separate.png" width="60%">

我们人类轻松地解决了这个分割问题，但是计算机程序正确分解图像是具有挑战性的。 一旦图像被分割，程序就需要对每个单独的数字进行分类。 举例来说，我们希望我们的程序能够识别出上面的第一个数字，

<img src="http://neuralnetworksanddeeplearning.com/images/mnist_first_digit.png">

认出是一个5。

我们将着重编写一个程序来解决第二个问题，即对个别数字进行分类。我们这样做是因为事实证明，一旦你有一种很好的分类单个数字的方法，分割问题就不难解决了。有很多方法可以解决分割问题。一种方法是尝试许多不同的分割图像的方法，使用个人数字分类器对每个试验分割进行评分。如果个体数字分类器对所有分段的分类信心满意，则试用分段得到高分;如果分类器在一个或多个分段中遇到很多问题，则分数较低。这个想法是，如果分类器在某处遇到麻烦，那么可能会遇到麻烦，因为分割选择不正确。这个想法和其他变化可以很好地用于解决分割问题。因此，不要担心分割，我们将专注于开发一个能够解决更加有趣和困难问题的神经网络，即识别单个手写数字。

要识别单个数字，我们将使用三层神经网络：

<img src="http://neuralnetworksanddeeplearning.com/images/tikz12.png">

网络的输入层包含编码输入像素值的神经元。 正如下一节所讨论的，我们的网络训练数据将包含许多扫描手写数字的28×28像素图像，因此输入层包含784 = 28×28 个神经元。 为了简单起见，我省略了上图中大部分的输入神经元。 输入像素是灰度，值为0.0表示白色，值为1.0表示黑色，值之间表示逐渐变暗的灰色阴影。

